{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Kmeans from scratch (in 3 lines of code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By gradually implementing the basic version of the Kmenas algorithm, we will practice our numpy skills, and also begin to introduce some ML concepts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### K-means algorithm loss function\n",
    "\n",
    "We've seen that the K-means algorithm aims to cluster the data into groups that minimize the following loss function \n",
    "\n",
    "$$L(C,I) = \\sum_{k=1}^K\\sum_{n\\in I_k} ||x_n - C_{k}||$$\n",
    "\n",
    "where \n",
    "1. $I_k$ is the index set of all samples in the $k$'th cluster\n",
    "2. $C_k$ is the centroid of the $k$'th cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means algorithm pseudo code\n",
    "\n",
    "We've also seen that the K-means algorithm tries to achieve its aim by iterating between the following two steps:\n",
    "1. **Given** groups' centers, **set** the samples of group k to be all the points closest to the group's center $C_k$ (we'll implement this in 2 lines of code)\n",
    "2. **Given** group's samples, **set** $C_k$ to be the mean of all points in the $k$'th group (we'll implement this in 1 line of code)\n",
    "\n",
    "\n",
    "A bit more formally, the following algorithm converges to a minimum of the loss function\n",
    "\n",
    "**Initialization:** \n",
    "1. Set $t=0$\n",
    "2. Set cluster centroids (randomly choose $K$ points from the data, or generate random samples) $$C^{(0)}_1, \\ldots, C^{(0)}_k \\in \\mathbb{R}^n$$\n",
    "\n",
    "**Iterate until convergence:**\n",
    "1. For each $n=1,2,\\ldots,N$ set $$c_n^{(t)} = \\text{argmin}_k || x_n - C_k^{(t)} ||$$ \n",
    "\n",
    "\n",
    "2. For each $k=1,2,\\ldots,K$ update cluster centroid $C_k^{(t)}$ to be the mean of all samples in the $k$'th cluster  $$C_k^{(t)} = \\frac{1}{\\left|I_k^{(t)}\\right|}\\sum_{n\\in I_{k}^{(t)}} x_n$$\n",
    "\n",
    "\n",
    "3. t=t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the algorithm according to the following steps:\n",
    "1. To implement step 1 above:\n",
    "    - compute the distances between every data point and all cluster centers\n",
    "    - given all distances between a point and all clusters centers, find the closest center\n",
    "2. To implement step 2 above:\n",
    "    - for each cluster, given its data points, compute it's mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the ground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin with importing some standard modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is for plotting and visualization only, and is not directly related to the algorithm, so we'll use it without discussing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Plot2dColoredSamples(X,Y,color=list('rgbckmy'),marker=['o'],PlotMeans = False):\n",
    "    plt.figure(figsize=[7,7])\n",
    "    uY = np.unique(Y)\n",
    "    K = len(uY)\n",
    "    for k in range(0,K):\n",
    "        if len(marker)==1:\n",
    "            plt.scatter(X[Y==uY[k],0], X[Y==uY[k],1], c=color[k], marker=marker[0], s=20)\n",
    "            if PlotMeans:\n",
    "                m = X[Y==k].mean(axis=0)\n",
    "                plt.scatter(m[0], m[1], c=color[k], marker='$ ' + str(k) + '$', s=200)\n",
    "        else:\n",
    "            plt.scatter(X[Y==uY[k],0], X[Y==uY[k],1], c=color[k], marker='$' + marker[k] + '$', s=20)\n",
    "            m = X[Y==k].mean(axis=0)\n",
    "            plt.scatter(m[0], m[1], c=color[k], marker='$ C_{' + str(k) + '}$', s=200)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll choose a utility function to compute the distance between all pairs of data points and cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist as distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function ``` distance``` to compute the distance between points ```X=[1,1]``` and ```Z=[0,0]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,1]])# distance expects a 2D array, and treats each row as a different data point\n",
    "print('X:')\n",
    "print(X)\n",
    "Z = np.array([[0,0]])\n",
    "print('\\nZ:')\n",
    "print(Z)\n",
    "d = distance(X,Z)\n",
    "print('\\ndistance(X,Z):')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets compute the distance between points ```X=[1,1]``` and every row of the array ```Z=[[0,0],[2,3]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,1]])\n",
    "print('X:')\n",
    "print(X)\n",
    "Z = np.array([[0,0],[2,3]])\n",
    "print('\\nZ:')\n",
    "print(Z)\n",
    "d = distance(X,Z)\n",
    "print('\\ndistance(X,Z):')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets compute the distance between each row of ```X=[[1,1],[-2,4],[2,4]]``` and every row of ```Z=[[0,0],[2,3]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,1],[-2,4],[2,4]])\n",
    "print('X:')\n",
    "print(X)\n",
    "Z = np.array([[0,0],[2,3]])\n",
    "print('\\nZ:')\n",
    "print(Z)\n",
    "d = distance(X,Z)\n",
    "print('\\ndistance(X,Z):')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute and print the Euclidean distances between each row of X and every row of Z, for each of the following\n",
    "1. $X=\\left[2,5\\right], Z = \\left[-1,4\\right]$\n",
    "\n",
    "2. $X=\\left[0.2,5.7\\right], Z = \\left[\\begin{array}{cc} \n",
    "0.2 & 1.5\\\\\n",
    "-2.1 & -0.08\n",
    "\\end{array}\\right]$\n",
    "\n",
    "3. $X=\\left[\\begin{array}{cc} \n",
    "0.12 & -3.4\\\\\n",
    "2.3 & -1.2\\\\\n",
    "-8.1 & 31.2\n",
    "\\end{array}\\right], Z = \\left[\\begin{array}{cc} \n",
    "0.2 & 1.5\\\\\n",
    "-2.1 & -0.08\n",
    "\\end{array}\\right]$\n",
    "\n",
    "4. Considering X and Z in section 3 above, for each row in X find the nearest (that is, with the smallest distance) row in Z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to compute the means of each cluster based on its samples, which the followng exercise helps us achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "1. Run the following python code, and explore a little the generated data to understand it better\n",
    "```python\n",
    "# Run this code in a code cell\n",
    "X = np.vstack((np.random.randn(10,3),np.random.randn(10,3)+5))\n",
    "Y = np.hstack((np.zeros(10),np.ones(10)))\n",
    "```\n",
    "2. Find the average row of X (that is, compute the mean of X along its columns)\n",
    "3. Find the average row of all of X's rows for which the corresponding Y is 0:\n",
    "    - first, select from X all the rows for which the corresponding Y is equal to 0\n",
    "    - then, compute the average row of these rows\n",
    "4. Similarly, find the average row of all of X's rows for which the corresponding Y is 1\n",
    "5. Implement 3+4 together in one line of code\n",
    "6. Explain how this code is related to the Kmeans algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now implement Kmeans in 3 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Copy the code below into a new code cell\n",
    "- Complete it using what we have learned above, to implement all steps of Kmeans\n",
    "- Apply it to the data above for each of the following number of clusters: K = 3,4,5,15 and explain the results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def BNHP_Kmeans(X, K, means=[], NumIterations = 20):\n",
    "    # Init: for simplicity, we'll take the first K data points to initialize the cluster means\n",
    "    # You can try replacing it with other initialization, and see what happens\n",
    "    means = X[:K]\n",
    "    # The code below implements the core logic of the Kmeans algorithm\n",
    "    for _ in range(NumIterations):\n",
    "        # line 1: compute the distance between each sample and every cluster center\n",
    "        distances = ...\n",
    "        # line 2: set the samples of group k to be all the points closest to Ck\n",
    "        labels = ...\n",
    "        # line 3: for each k set Ck to be the mean of all points in group k \n",
    "        means = ...\n",
    "    return means, labels     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing your implementation of Kmeans, you can use the following code to visualize your clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(2500,2)*10\n",
    "Plot2dColoredSamples(X,np.ones(X.shape[0]),color='k')\n",
    "plt.title('How will Kmeans cluster this data for K=3,4,5,15?',color='g',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing your implementation of Kmeans, you can use the following code to visualize your clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "color=list('rgbcmy')*5\n",
    "centroids, labels = BNHP_Kmeans(X, K)\n",
    "Plot2dColoredSamples(X,labels,color=color,marker=[str(s) for s in np.unique(labels)])\n",
    "plt.title('Clustering results for number of clusters K = ' +str(K));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the data into K= 3, 4, 5, 15 groups and explain the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the data is not ditributed uniformly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xdense  = np.random.uniform(0, 10, 450).reshape(225,2)\n",
    "Xsparse = np.hstack((np.random.uniform(10, 30, 25).reshape(25,1),np.random.uniform(0, 10, 25).reshape(25,1)))\n",
    "X = np.vstack((Xdense,Xsparse))\n",
    "Plot2dColoredSamples(X=X,Y=np.ones(X.shape[0]),color=list('k'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our Kmeans on this data for various values of $K=3, 6, 9, 12, 20$ and explain the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Compute the distortion for clustering our data above into K=3 clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K=3\n",
    "means, labels = BNHP_Kmeans(X, K)\n",
    "Plot2dColoredSamples(X,labels,color=color,marker=[str(s) for s in np.unique(labels)])\n",
    "plt.title('Clustering results for number of clusters K = ' +str(K))\n",
    "    \n",
    "ClustersDistortion = [distance(X[labels==k],means[k].reshape(1,2)).sum() for k in range(K)]# means[k].reshape(1,2) so that it is a 2D array; assuming every cluster has more than one point so that X[labels==k] is already 2D \n",
    "ClustersDistortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to be one of the results our Kmeans implementation returns and verify it works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we've avhieved two main goals \n",
    "1. practiced our numpy skills\n",
    "2. discussed the unsupervised ML problem of clustering, and studied the canonical Kmeans algorithm by discussing the theory that underlies it, implementing it in code, and applying it to some data we've generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
